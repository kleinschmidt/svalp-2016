---
bibliography: /Users/dkleinschmidt/Documents/papers/library-clean.bib
csl: apa.csl
output: pdf_document
geometry: margin=1in
fig_caption: true
graphics: yes

---

```{r, echo=FALSE, results='hide'}

library(supunsup)
library(dplyr)
library(tidyr)
library(magrittr)

data <- supunsup::supunsup_clean %>%
  filter(supCond == 'unsupervised') %>%
  mutate(trueCat = respCategory,
         subjNum = as.numeric(factor(subject)),
         trueCatNum = as.numeric(trueCat),
         respCatNum = as.numeric(respCat))

n_subj <- data %>% group_by(subject) %>% summarise() %>% tally()

knitr::opts_chunk$set(echo=FALSE,
                      results='hide',
                      message=FALSE,
                      warning=FALSE,
                      error=FALSE)

library(ggplot2)
theme_set(theme_bw())


```

One of the longest-standing puzzles in human speech perception is how listeners manage to cope with the often extreme differences in how individual talkers use acoustic cues to realize their linguistic intentions.  A number of solutions have been proposed, including the recent proposal that listeners quickly _adapt_ to unfamiliar talkers by _learning_ the distributions of acoustic cues that they produce (their "accent").

This can be formalized as a kind of statistical inference, where listeners try to infer which of all possible accents best explains a talker's speech [@Kleinschmidt2015]. Prior experience helps because it narrows down the range of possibilities that a listener needs to consider (in Bayesian jargon, it provides an _informative prior_ on accents). We test a critical prediction of this view: when an unfamiliar talker's accent falls _outside_ the range of typical variation across talkers, listeners should adapt only partially. Specifically, listeners' phonetic classifications should reflect a compromise between listeners' prior expectations and the actual accent they hear. We also, in doing so, demonstrate a novel technique for measuring listeners' subjective prior expectations about an unfamiliar talker's accent.

We use a /b/-/p/ VOT distributional learning paradigm [@Clayards2008], where listeners ($n = `r n_subj`$) hear a bimodal distribution over an acoustic cue (VOT), with a cluster at a low value implicitly corresponding to /b/ and another at a high value corresponding to /p/.  By varying the location of these clusters, we create accents that are more or less like those produced by a typical American English talker [as measured by, e.g., @Kronrod2012].

```{r input-vs-prior-stats, fig.width=8, fig.height=2, out.width='\\textwidth', fig.cap="VOT distributions for each accent."}

## copied from NIPS paper #######################################

## prior parameters from Kronrod et al. (CogSci 2012)
prior_stats <- data.frame(category=factor(c('b', 'p')),
                          mean = c(0, 60),
                          sd = sqrt(c(14, 254)))

exposure_stats <- data %>%
  group_by(bvotCond, category=trueCat) %>%
  summarise(mean=mean(vot), sd=sd(vot))

sd_noise = sqrt(82)

stats_to_lhood <- function(stats, noise_sd=sd_noise) {
  stats %>%
    group_by(category, mean, sd) %>%
    do(data.frame(vot=seq(-30, 90, 0.5))) %>%
    ungroup() %>%
    mutate(lhood = dnorm(vot, mean, sqrt(sd^2 + noise_sd^2))) %>%
    select(-mean, -sd)
}

exposure_lhood <- exposure_stats %>%
  group_by(bvotCond) %>%
  do(stats_to_lhood(., sd_noise))

prior_lhood <- prior_stats %>% stats_to_lhood(sd_noise)

data %>%
  group_by(bvotCond, vot) %>%
  filter(subject == first(subject)) %>%
  tally() %>%
  ggplot(aes(x=vot)) +
  geom_bar(stat='identity', aes(y=n, fill=bvotCond)) +
  geom_line(data=prior_lhood, aes(y=lhood*1600, group=category),
            color="black", linetype=2) +
  geom_text(data=data.frame(bvotCond=-10), x = 10, y = 60,
            label = 'Typical Talker',
            color='black', hjust=0, vjust=0.3, size=3) +
  geom_text(data=data.frame(bvotCond=-10), x = 40, y = 50,
            label = 'Exposure\nTalker',
            color=hcl(h=15, c=100, l=65), hjust=0, vjust=0.8, size=3,
            lineheight=1) + 
  facet_grid(.~bvotCond) +
  scale_x_continuous('VOT (ms)') +
  scale_y_continuous('Frequency') +
  scale_fill_discrete('/b/ mean\nVOT') +
  theme(legend.position='none')

```

We measure how well listeners _learn_ these accents by comparing their classification functions to the ideal boundaries implied by the exposure distributions alone.  As predicted, when the VOT clusters were unusually high or low, listeners _actual_ category boundaries reflected a compromise between the boundary of a typical talker and the exposure talker's distributions.

![After exposure, listeners' /b/-/p/ classifications (thin lines) reflected a compromise between the typical (dashed black) and experimental (dashed colored).
](../nips_2015/kleinschmidt_infer_priors_files/figure-latex/supunsup-belief-updating-qualitative-1.pdf)

Second, we used a belief-updating model to work backwards from the patterns of adaptation to different accents, inferring what listeners' starting beliefs were, and how confident they were in those beliefs.  The inferred prior expectations matched the range of typical American English talkers' /b/ and /p/ distributions, including the counterintuitive finding that listeners were _more_ uncertain about the /b/ mean VOT than /p/, corresponding to the fact that there's high variance in the VOT of /b/ _across_ talkers due to some talkers prevoicing [@Lisker1964].

The ability to measure listeners' prior expectations potentially provides an important and heretofore missing tool in the toolbox of sociophonetics: it directly links the variability in _production_ of linguistic variables with listeners' subjective expectations about those variables, both conditioned on _social_ variables.  Our proof-of-concept here uses standard American English as a reference, but the same procedure can be applied to more specific variables like gender, region, class, etc., by providing information to the listener about _who_ the talker is [which listeners do use to guide speech perception, @Hay2010; @Niedzielski1999; @Strand1996].


# References
